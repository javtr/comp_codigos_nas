{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Codigo                                              Resto\n",
      "0     VR F35mm 851242                                       QUÉ HORA ES?\n",
      "1     VR F35mm 851304                             A Orillas Del Amazonas\n",
      "2     VR F35mm 851303                                  Ángulos De Bogotá\n",
      "3     VR F35mm 851011                      Batalla del pantano de Vargas\n",
      "4     VR F35mm 851112                         Cali, la Sultana del Valle\n",
      "...               ...                                                ...\n",
      "1324  VR F16mm 802705                   Yuruparí Cap Cumbia sobre el río\n",
      "1325  VR F16mm 802706               Yuruparí Cap Las farotas de Talaigua\n",
      "1326  VR F16mm 802707  Yuruparí Cap Mompox, el ocaso del oro y del barro\n",
      "1327  VR F16mm 802708               Yuruparí Cap El Cristo negro de Tadó\n",
      "1328  VR F16mm 802709                Yuruparí Cap El Carnaval del Diablo\n",
      "\n",
      "[1329 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo Excel\n",
    "file_path = 'data/original/Ubicación actual de las Versiones Restauradas.xlsx'\n",
    "\n",
    "# Especifica el nombre o índice de la hoja que deseas importar\n",
    "sheet_name = 'VERSIONES RESTAURADAS'  # También puedes usar el índice de la hoja, por ejemplo: 0\n",
    "\n",
    "# Cargar la hoja especificada en un DataFrame\n",
    "df_verificacion = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "df_codigos = pd.DataFrame()\n",
    "\n",
    "df_codigos[['Codigo', 'Resto']] = df_verificacion['Código versión restaurada'].str.extract(r'(^.*\\d{6})\\s*(.*)')\n",
    "\n",
    "# Mostrar el nuevo DataFrame con las dos columnas\n",
    "print(df_codigos[['Codigo', 'Resto']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def limpiar_y_validar_codigo(codigo):\n",
    "#     if pd.isna(codigo):\n",
    "#         return None\n",
    "#     try:\n",
    "#         codigo_str = str(codigo).strip()\n",
    "#         partes = codigo_str.split()\n",
    "#         if len(partes) >= 3:\n",
    "#             return ' '.join(partes[:3])\n",
    "#     except:\n",
    "#         pass\n",
    "#     return None\n",
    "\n",
    "# def procesar_archivos_csv(ruta_carpeta, df_codigos):\n",
    "#     # Limpiar y validar los códigos\n",
    "#     codigos_validos = df_codigos.iloc[:, 0].apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Crear un diccionario para almacenar los códigos a buscar\n",
    "#     codigos_dict = {codigo: idx for idx, codigo in enumerate(codigos_validos) if codigo is not None}\n",
    "    \n",
    "#     # Inicializar las nuevas columnas\n",
    "#     df_codigos['existente'] = 'no'\n",
    "#     df_codigos['archivo'] = ''\n",
    "#     df_codigos['fila'] = -1\n",
    "#     df_codigos['extension'] = ''\n",
    "    \n",
    "#     # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "#     if ruta_carpeta.endswith('.xlsx'):\n",
    "#         # Si es un archivo Excel, procesarlo directamente\n",
    "#         df = pd.read_excel(ruta_carpeta)\n",
    "#         procesar_dataframe(df, codigos_dict, df_codigos, os.path.basename(ruta_carpeta))\n",
    "#     else:\n",
    "#         # Si es una carpeta, procesar todos los archivos CSV\n",
    "#         archivos_csv = glob(os.path.join(ruta_carpeta, '*.csv'))\n",
    "#         for archivo in archivos_csv:\n",
    "#             # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "#             for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "#                 procesar_dataframe(chunk, codigos_dict, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "#     return df_codigos\n",
    "\n",
    "# def procesar_dataframe(df, codigos_dict, df_codigos, nombre_archivo):\n",
    "#     # Extraer los códigos de la columna 'contenido'\n",
    "#     df['codigo'] = df['contenido'].astype(str).apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Filtrar las filas que contienen códigos de interés\n",
    "#     mask = df['codigo'].isin(codigos_dict.keys())\n",
    "#     matches = df[mask]\n",
    "    \n",
    "#     if not matches.empty:\n",
    "#         for _, row in matches.iterrows():\n",
    "#             idx = codigos_dict[row['codigo']]\n",
    "#             if df_codigos.at[idx, 'existente'] == 'no':\n",
    "#                 df_codigos.at[idx, 'existente'] = 'si'\n",
    "#                 df_codigos.at[idx, 'archivo'] = nombre_archivo\n",
    "#                 df_codigos.at[idx, 'fila'] = row.name\n",
    "#                 df_codigos.at[idx, 'extension'] = row['extension']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_archivo = 'data/original/nas_csv'\n",
    "\n",
    "# df_codigos_actualizado = procesar_archivos_csv(ruta_archivo, df_codigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from glob import glob\n",
    "\n",
    "# def limpiar_y_validar_codigo(codigo):\n",
    "#     if pd.isna(codigo):\n",
    "#         return None\n",
    "#     try:\n",
    "#         codigo_str = str(codigo).strip()\n",
    "#         partes = codigo_str.split()\n",
    "#         return ' '.join(partes[:3]) if len(partes) >= 3 else None\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# def procesar_archivos_csv2(ruta, df_codigos):\n",
    "#     # Limpiar y validar los códigos\n",
    "#     codigos_validos = df_codigos.iloc[:, 0].apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Crear un diccionario para almacenar los códigos a buscar\n",
    "#     codigos_dict = {codigo: idx for idx, codigo in enumerate(codigos_validos) if codigo is not None}\n",
    "    \n",
    "#     # Inicializar las nuevas columnas\n",
    "#     df_codigos['existente'] = 'no'\n",
    "#     df_codigos['archivo'] = ''\n",
    "#     df_codigos['fila'] = -1\n",
    "#     df_codigos['extension'] = ''\n",
    "    \n",
    "#     # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "#     if ruta.endswith('.xlsx'):\n",
    "#         # Si es un archivo Excel, procesarlo directamente\n",
    "#         df = pd.read_excel(ruta)\n",
    "#         procesar_dataframe(df, codigos_dict, df_codigos, os.path.basename(ruta))\n",
    "#     else:\n",
    "#         # Si es una carpeta, procesar todos los archivos CSV\n",
    "#         archivos_csv = glob(os.path.join(ruta, '*.csv'))\n",
    "#         for archivo in archivos_csv:\n",
    "#             # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "#             for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "#                 procesar_dataframe(chunk, codigos_dict, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "#     return df_codigos\n",
    "\n",
    "# def procesar_dataframe(df, codigos_dict, df_codigos, nombre_archivo):\n",
    "#     # Extraer los códigos de la columna 'contenido'\n",
    "#     df['codigo'] = df['contenido'].astype(str).apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Filtrar las filas que contienen códigos de interés\n",
    "#     matches = df[df['codigo'].isin(codigos_dict)]\n",
    "    \n",
    "#     for _, row in matches.iterrows():\n",
    "#         idx = codigos_dict[row['codigo']]\n",
    "#         if df_codigos.at[idx, 'existente'] == 'no':\n",
    "#             df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension']] = ['si', nombre_archivo, row.name, row['extension']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_archivo = 'data/original/nas_csv'\n",
    "\n",
    "# df_codigos_actualizado = procesar_archivos_csv2(ruta_archivo, df_codigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from glob import glob\n",
    "# import re\n",
    "\n",
    "# def limpiar_y_validar_codigo(codigo):\n",
    "#     if pd.isna(codigo):\n",
    "#         return None\n",
    "#     try:\n",
    "#         codigo_str = str(codigo).strip().upper()\n",
    "#         # Patrón más flexible para capturar diferentes formatos de código\n",
    "#         patron = r'^((?:\\w+\\s+){1,3}\\d+)'\n",
    "#         match = re.match(patron, codigo_str)\n",
    "#         if match:\n",
    "#             return match.group(1).strip()\n",
    "#     except:\n",
    "#         pass\n",
    "#     return None\n",
    "\n",
    "# def procesar_archivos(ruta, df_codigos):\n",
    "#     # Limpiar y validar los códigos\n",
    "#     codigos_validos = df_codigos.iloc[:, 0].apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Crear un diccionario para almacenar los códigos a buscar\n",
    "#     codigos_dict = {codigo: idx for idx, codigo in enumerate(codigos_validos) if codigo is not None}\n",
    "    \n",
    "#     # Inicializar las nuevas columnas\n",
    "#     df_codigos['existente'] = 'no'\n",
    "#     df_codigos['archivo'] = ''\n",
    "#     df_codigos['fila'] = -1\n",
    "#     df_codigos['extension'] = ''\n",
    "    \n",
    "#     # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "#     if ruta.endswith('.xlsx'):\n",
    "#         # Si es un archivo Excel, procesarlo directamente\n",
    "#         df = pd.read_excel(ruta)\n",
    "#         procesar_dataframe(df, codigos_dict, df_codigos, os.path.basename(ruta))\n",
    "#     else:\n",
    "#         # Si es una carpeta, procesar todos los archivos CSV\n",
    "#         archivos_csv = glob(os.path.join(ruta, '*.csv'))\n",
    "#         for archivo in archivos_csv:\n",
    "#             # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "#             for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "#                 procesar_dataframe(chunk, codigos_dict, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "#     return df_codigos\n",
    "\n",
    "# def procesar_dataframe(df, codigos_dict, df_codigos, nombre_archivo):\n",
    "#     # Extraer los códigos de la columna 'contenido'\n",
    "#     df['codigo'] = df['contenido'].astype(str).apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Filtrar las filas que contienen códigos de interés\n",
    "#     for codigo, idx in codigos_dict.items():\n",
    "#         matches = df[df['codigo'].str.contains(codigo, regex=False, na=False)]\n",
    "        \n",
    "#         for _, row in matches.iterrows():\n",
    "#             if df_codigos.at[idx, 'existente'] == 'no':\n",
    "#                 df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension']] = ['si', nombre_archivo, row.name, row['extension']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from glob import glob\n",
    "# import re\n",
    "\n",
    "# def limpiar_y_validar_codigo(codigo):\n",
    "#     if pd.isna(codigo):\n",
    "#         return None\n",
    "#     try:\n",
    "#         codigo_str = str(codigo).strip().upper()\n",
    "#         # Patrón más flexible para capturar diferentes formatos de código\n",
    "#         patron = r'^((?:\\w+\\s+){1,3}\\d+)'\n",
    "#         match = re.match(patron, codigo_str)\n",
    "#         if match:\n",
    "#             return match.group(1).strip()\n",
    "#     except:\n",
    "#         pass\n",
    "#     return None\n",
    "\n",
    "# def procesar_archivos(ruta, df_codigos):\n",
    "#     # Limpiar y validar los códigos\n",
    "#     codigos_validos = df_codigos.iloc[:, 0].apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Crear un diccionario para almacenar los códigos a buscar\n",
    "#     codigos_dict = {codigo: idx for idx, codigo in enumerate(codigos_validos) if codigo is not None}\n",
    "    \n",
    "#     # Inicializar las nuevas columnas\n",
    "#     df_codigos['existente'] = 'no'\n",
    "#     df_codigos['archivo'] = ''\n",
    "#     df_codigos['fila'] = -1\n",
    "#     df_codigos['extension'] = ''\n",
    "    \n",
    "#     # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "#     if ruta.endswith('.xlsx'):\n",
    "#         # Si es un archivo Excel, procesarlo directamente\n",
    "#         df = pd.read_excel(ruta)\n",
    "#         procesar_dataframe(df, codigos_dict, df_codigos, os.path.basename(ruta))\n",
    "#     else:\n",
    "#         # Si es una carpeta, procesar todos los archivos CSV\n",
    "#         archivos_csv = glob(os.path.join(ruta, '*.csv'))\n",
    "#         for archivo in archivos_csv:\n",
    "#             # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "#             for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "#                 procesar_dataframe(chunk, codigos_dict, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "#     return df_codigos\n",
    "\n",
    "# def procesar_dataframe(df, codigos_dict, df_codigos, nombre_archivo):\n",
    "#     # Convertir la columna 'contenido' a mayúsculas para comparación insensible a mayúsculas/minúsculas\n",
    "#     df['contenido_upper'] = df['contenido'].astype(str).str.upper()\n",
    "    \n",
    "#     for codigo, idx in codigos_dict.items():\n",
    "#         # Usar una expresión regular para buscar el código al inicio de la cadena\n",
    "#         patron = f\"^{re.escape(codigo)}\\\\b\"\n",
    "#         matches = df[df['contenido_upper'].str.contains(patron, regex=True, na=False)]\n",
    "        \n",
    "#         for _, row in matches.iterrows():\n",
    "#             if df_codigos.at[idx, 'existente'] == 'no':\n",
    "#                 df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension']] = ['si', nombre_archivo, row.name, row['extension']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_codigo(codigo, cadena):\n",
    "    # Asegurarse de que ambos sean cadenas antes de aplicar strip() y upper()\n",
    "    if pd.isna(codigo) or not isinstance(codigo, str):\n",
    "        return False  # Omitir si el código no es una cadena\n",
    "    if pd.isna(cadena) or not isinstance(cadena, str):\n",
    "        return False  # Omitir si la cadena no es válida\n",
    "    \n",
    "    # Convertir a mayúsculas y eliminar espacios en blanco\n",
    "    return codigo.strip().upper() in cadena.strip().upper()\n",
    "\n",
    "\n",
    "\n",
    "def procesar_archivos(ruta, df_codigos):\n",
    "    # Inicializar las nuevas columnas\n",
    "    df_codigos['existente'] = 'no'\n",
    "    df_codigos['archivo'] = ''\n",
    "    df_codigos['fila'] = -1\n",
    "    df_codigos['extension'] = ''\n",
    "    \n",
    "    # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "    if ruta.endswith('.xlsx'):\n",
    "        # Si es un archivo Excel, procesarlo directamente\n",
    "        df = pd.read_excel(ruta)\n",
    "        procesar_dataframe(df, df_codigos, os.path.basename(ruta))\n",
    "    else:\n",
    "        # Si es una carpeta, procesar todos los archivos CSV\n",
    "        archivos_csv = glob(os.path.join(ruta, '*.csv'))\n",
    "        for archivo in archivos_csv:\n",
    "            # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "            for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "                procesar_dataframe(chunk, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "    return df_codigos\n",
    "\n",
    "\n",
    "def procesar_dataframe(df, df_codigos, nombre_archivo):\n",
    "    for idx, row_codigos in df_codigos.iterrows():\n",
    "        codigo = row_codigos['Codigo']\n",
    "        matches = df[df['contenido'].apply(lambda x: buscar_codigo(codigo, x))]\n",
    "        \n",
    "        if not matches.empty:\n",
    "            primera_coincidencia = matches.iloc[0]\n",
    "            df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension']] = [\n",
    "                'si', \n",
    "                nombre_archivo, \n",
    "                primera_coincidencia.name, \n",
    "                primera_coincidencia['extension']\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 1531, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ruta_archivo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/original/nas_csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df_codigos_actualizado \u001b[38;5;241m=\u001b[39m \u001b[43mprocesar_archivos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_archivo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_codigos\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m, in \u001b[0;36mprocesar_archivos\u001b[0;34m(ruta, df_codigos)\u001b[0m\n\u001b[1;32m     27\u001b[0m     archivos_csv \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ruta, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m archivo \u001b[38;5;129;01min\u001b[39;00m archivos_csv:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Leer el archivo CSV en chunks para optimizar la memoria\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(archivo, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenido\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextension\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     31\u001b[0m             procesar_dataframe(chunk, df_codigos, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(archivo))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_codigos\n",
      "File \u001b[0;32m~/Desarrollo/comparacion-codigos-nas/my_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desarrollo/comparacion-codigos-nas/my_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desarrollo/comparacion-codigos-nas/my_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desarrollo/comparacion-codigos-nas/my_env/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 1531, saw 3\n"
     ]
    }
   ],
   "source": [
    "ruta_archivo = 'data/original/nas_csv'\n",
    "\n",
    "df_codigos_actualizado = procesar_archivos(ruta_archivo, df_codigos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
