{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Codigo                                              Resto\n",
      "0     VR F35mm 851242                                       QUÉ HORA ES?\n",
      "1     VR F35mm 851304                             A Orillas Del Amazonas\n",
      "2     VR F35mm 851303                                  Ángulos De Bogotá\n",
      "3     VR F35mm 851011                      Batalla del pantano de Vargas\n",
      "4     VR F35mm 851112                         Cali, la Sultana del Valle\n",
      "...               ...                                                ...\n",
      "1324  VR F16mm 802705                   Yuruparí Cap Cumbia sobre el río\n",
      "1325  VR F16mm 802706               Yuruparí Cap Las farotas de Talaigua\n",
      "1326  VR F16mm 802707  Yuruparí Cap Mompox, el ocaso del oro y del barro\n",
      "1327  VR F16mm 802708               Yuruparí Cap El Cristo negro de Tadó\n",
      "1328  VR F16mm 802709                Yuruparí Cap El Carnaval del Diablo\n",
      "\n",
      "[1329 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo Excel\n",
    "file_path = 'data/original/Ubicación actual de las Versiones Restauradas.xlsx'\n",
    "\n",
    "# Especifica el nombre o índice de la hoja que deseas importar\n",
    "sheet_name = 'VERSIONES RESTAURADAS'  # También puedes usar el índice de la hoja, por ejemplo: 0\n",
    "\n",
    "# Cargar la hoja especificada en un DataFrame\n",
    "df_verificacion = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "df_codigos = pd.DataFrame()\n",
    "\n",
    "df_codigos[['Codigo', 'Resto']] = df_verificacion['Código versión restaurada'].str.extract(r'(^.*\\d{6})\\s*(.*)')\n",
    "\n",
    "# Mostrar el nuevo DataFrame con las dos columnas\n",
    "print(df_codigos[['Codigo', 'Resto']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def limpiar_y_validar_codigo(codigo):\n",
    "#     if pd.isna(codigo):\n",
    "#         return None\n",
    "#     try:\n",
    "#         codigo_str = str(codigo).strip()\n",
    "#         partes = codigo_str.split()\n",
    "#         if len(partes) >= 3:\n",
    "#             return ' '.join(partes[:3])\n",
    "#     except:\n",
    "#         pass\n",
    "#     return None\n",
    "\n",
    "# def procesar_archivos_csv(ruta_carpeta, df_codigos):\n",
    "#     # Limpiar y validar los códigos\n",
    "#     codigos_validos = df_codigos.iloc[:, 0].apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Crear un diccionario para almacenar los códigos a buscar\n",
    "#     codigos_dict = {codigo: idx for idx, codigo in enumerate(codigos_validos) if codigo is not None}\n",
    "    \n",
    "#     # Inicializar las nuevas columnas\n",
    "#     df_codigos['existente'] = 'no'\n",
    "#     df_codigos['archivo'] = ''\n",
    "#     df_codigos['fila'] = -1\n",
    "#     df_codigos['extension'] = ''\n",
    "    \n",
    "#     # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "#     if ruta_carpeta.endswith('.xlsx'):\n",
    "#         # Si es un archivo Excel, procesarlo directamente\n",
    "#         df = pd.read_excel(ruta_carpeta)\n",
    "#         procesar_dataframe(df, codigos_dict, df_codigos, os.path.basename(ruta_carpeta))\n",
    "#     else:\n",
    "#         # Si es una carpeta, procesar todos los archivos CSV\n",
    "#         archivos_csv = glob(os.path.join(ruta_carpeta, '*.csv'))\n",
    "#         for archivo in archivos_csv:\n",
    "#             # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "#             for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "#                 procesar_dataframe(chunk, codigos_dict, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "#     return df_codigos\n",
    "\n",
    "# def procesar_dataframe(df, codigos_dict, df_codigos, nombre_archivo):\n",
    "#     # Extraer los códigos de la columna 'contenido'\n",
    "#     df['codigo'] = df['contenido'].astype(str).apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Filtrar las filas que contienen códigos de interés\n",
    "#     mask = df['codigo'].isin(codigos_dict.keys())\n",
    "#     matches = df[mask]\n",
    "    \n",
    "#     if not matches.empty:\n",
    "#         for _, row in matches.iterrows():\n",
    "#             idx = codigos_dict[row['codigo']]\n",
    "#             if df_codigos.at[idx, 'existente'] == 'no':\n",
    "#                 df_codigos.at[idx, 'existente'] = 'si'\n",
    "#                 df_codigos.at[idx, 'archivo'] = nombre_archivo\n",
    "#                 df_codigos.at[idx, 'fila'] = row.name\n",
    "#                 df_codigos.at[idx, 'extension'] = row['extension']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_archivo = 'data/original/nas_csv'\n",
    "\n",
    "# df_codigos_actualizado = procesar_archivos_csv(ruta_archivo, df_codigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from glob import glob\n",
    "\n",
    "# def limpiar_y_validar_codigo(codigo):\n",
    "#     if pd.isna(codigo):\n",
    "#         return None\n",
    "#     try:\n",
    "#         codigo_str = str(codigo).strip()\n",
    "#         partes = codigo_str.split()\n",
    "#         return ' '.join(partes[:3]) if len(partes) >= 3 else None\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# def procesar_archivos_csv2(ruta, df_codigos):\n",
    "#     # Limpiar y validar los códigos\n",
    "#     codigos_validos = df_codigos.iloc[:, 0].apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Crear un diccionario para almacenar los códigos a buscar\n",
    "#     codigos_dict = {codigo: idx for idx, codigo in enumerate(codigos_validos) if codigo is not None}\n",
    "    \n",
    "#     # Inicializar las nuevas columnas\n",
    "#     df_codigos['existente'] = 'no'\n",
    "#     df_codigos['archivo'] = ''\n",
    "#     df_codigos['fila'] = -1\n",
    "#     df_codigos['extension'] = ''\n",
    "    \n",
    "#     # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "#     if ruta.endswith('.xlsx'):\n",
    "#         # Si es un archivo Excel, procesarlo directamente\n",
    "#         df = pd.read_excel(ruta)\n",
    "#         procesar_dataframe(df, codigos_dict, df_codigos, os.path.basename(ruta))\n",
    "#     else:\n",
    "#         # Si es una carpeta, procesar todos los archivos CSV\n",
    "#         archivos_csv = glob(os.path.join(ruta, '*.csv'))\n",
    "#         for archivo in archivos_csv:\n",
    "#             # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "#             for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "#                 procesar_dataframe(chunk, codigos_dict, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "#     return df_codigos\n",
    "\n",
    "# def procesar_dataframe(df, codigos_dict, df_codigos, nombre_archivo):\n",
    "#     # Extraer los códigos de la columna 'contenido'\n",
    "#     df['codigo'] = df['contenido'].astype(str).apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Filtrar las filas que contienen códigos de interés\n",
    "#     matches = df[df['codigo'].isin(codigos_dict)]\n",
    "    \n",
    "#     for _, row in matches.iterrows():\n",
    "#         idx = codigos_dict[row['codigo']]\n",
    "#         if df_codigos.at[idx, 'existente'] == 'no':\n",
    "#             df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension']] = ['si', nombre_archivo, row.name, row['extension']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_archivo = 'data/original/nas_csv'\n",
    "\n",
    "# df_codigos_actualizado = procesar_archivos_csv2(ruta_archivo, df_codigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from glob import glob\n",
    "# import re\n",
    "\n",
    "# def limpiar_y_validar_codigo(codigo):\n",
    "#     if pd.isna(codigo):\n",
    "#         return None\n",
    "#     try:\n",
    "#         codigo_str = str(codigo).strip().upper()\n",
    "#         # Patrón más flexible para capturar diferentes formatos de código\n",
    "#         patron = r'^((?:\\w+\\s+){1,3}\\d+)'\n",
    "#         match = re.match(patron, codigo_str)\n",
    "#         if match:\n",
    "#             return match.group(1).strip()\n",
    "#     except:\n",
    "#         pass\n",
    "#     return None\n",
    "\n",
    "# def procesar_archivos(ruta, df_codigos):\n",
    "#     # Limpiar y validar los códigos\n",
    "#     codigos_validos = df_codigos.iloc[:, 0].apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Crear un diccionario para almacenar los códigos a buscar\n",
    "#     codigos_dict = {codigo: idx for idx, codigo in enumerate(codigos_validos) if codigo is not None}\n",
    "    \n",
    "#     # Inicializar las nuevas columnas\n",
    "#     df_codigos['existente'] = 'no'\n",
    "#     df_codigos['archivo'] = ''\n",
    "#     df_codigos['fila'] = -1\n",
    "#     df_codigos['extension'] = ''\n",
    "    \n",
    "#     # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "#     if ruta.endswith('.xlsx'):\n",
    "#         # Si es un archivo Excel, procesarlo directamente\n",
    "#         df = pd.read_excel(ruta)\n",
    "#         procesar_dataframe(df, codigos_dict, df_codigos, os.path.basename(ruta))\n",
    "#     else:\n",
    "#         # Si es una carpeta, procesar todos los archivos CSV\n",
    "#         archivos_csv = glob(os.path.join(ruta, '*.csv'))\n",
    "#         for archivo in archivos_csv:\n",
    "#             # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "#             for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "#                 procesar_dataframe(chunk, codigos_dict, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "#     return df_codigos\n",
    "\n",
    "# def procesar_dataframe(df, codigos_dict, df_codigos, nombre_archivo):\n",
    "#     # Extraer los códigos de la columna 'contenido'\n",
    "#     df['codigo'] = df['contenido'].astype(str).apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Filtrar las filas que contienen códigos de interés\n",
    "#     for codigo, idx in codigos_dict.items():\n",
    "#         matches = df[df['codigo'].str.contains(codigo, regex=False, na=False)]\n",
    "        \n",
    "#         for _, row in matches.iterrows():\n",
    "#             if df_codigos.at[idx, 'existente'] == 'no':\n",
    "#                 df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension']] = ['si', nombre_archivo, row.name, row['extension']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from glob import glob\n",
    "# import re\n",
    "\n",
    "# def limpiar_y_validar_codigo(codigo):\n",
    "#     if pd.isna(codigo):\n",
    "#         return None\n",
    "#     try:\n",
    "#         codigo_str = str(codigo).strip().upper()\n",
    "#         # Patrón más flexible para capturar diferentes formatos de código\n",
    "#         patron = r'^((?:\\w+\\s+){1,3}\\d+)'\n",
    "#         match = re.match(patron, codigo_str)\n",
    "#         if match:\n",
    "#             return match.group(1).strip()\n",
    "#     except:\n",
    "#         pass\n",
    "#     return None\n",
    "\n",
    "# def procesar_archivos(ruta, df_codigos):\n",
    "#     # Limpiar y validar los códigos\n",
    "#     codigos_validos = df_codigos.iloc[:, 0].apply(limpiar_y_validar_codigo)\n",
    "    \n",
    "#     # Crear un diccionario para almacenar los códigos a buscar\n",
    "#     codigos_dict = {codigo: idx for idx, codigo in enumerate(codigos_validos) if codigo is not None}\n",
    "    \n",
    "#     # Inicializar las nuevas columnas\n",
    "#     df_codigos['existente'] = 'no'\n",
    "#     df_codigos['archivo'] = ''\n",
    "#     df_codigos['fila'] = -1\n",
    "#     df_codigos['extension'] = ''\n",
    "    \n",
    "#     # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "#     if ruta.endswith('.xlsx'):\n",
    "#         # Si es un archivo Excel, procesarlo directamente\n",
    "#         df = pd.read_excel(ruta)\n",
    "#         procesar_dataframe(df, codigos_dict, df_codigos, os.path.basename(ruta))\n",
    "#     else:\n",
    "#         # Si es una carpeta, procesar todos los archivos CSV\n",
    "#         archivos_csv = glob(os.path.join(ruta, '*.csv'))\n",
    "#         for archivo in archivos_csv:\n",
    "#             # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "#             for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "#                 procesar_dataframe(chunk, codigos_dict, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "#     return df_codigos\n",
    "\n",
    "# def procesar_dataframe(df, codigos_dict, df_codigos, nombre_archivo):\n",
    "#     # Convertir la columna 'contenido' a mayúsculas para comparación insensible a mayúsculas/minúsculas\n",
    "#     df['contenido_upper'] = df['contenido'].astype(str).str.upper()\n",
    "    \n",
    "#     for codigo, idx in codigos_dict.items():\n",
    "#         # Usar una expresión regular para buscar el código al inicio de la cadena\n",
    "#         patron = f\"^{re.escape(codigo)}\\\\b\"\n",
    "#         matches = df[df['contenido_upper'].str.contains(patron, regex=True, na=False)]\n",
    "        \n",
    "#         for _, row in matches.iterrows():\n",
    "#             if df_codigos.at[idx, 'existente'] == 'no':\n",
    "#                 df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension']] = ['si', nombre_archivo, row.name, row['extension']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def buscar_codigo(codigo, cadena):\n",
    "#     # Asegurarse de que ambos sean cadenas antes de aplicar strip() y upper()\n",
    "#     if pd.isna(codigo) or not isinstance(codigo, str):\n",
    "#         return False  # Omitir si el código no es una cadena\n",
    "#     if pd.isna(cadena) or not isinstance(cadena, str):\n",
    "#         return False  # Omitir si la cadena no es válida\n",
    "    \n",
    "#     # Convertir a mayúsculas y eliminar espacios en blanco\n",
    "#     return codigo.strip().upper() in cadena.strip().upper()\n",
    "\n",
    "# def procesar_archivos(ruta, df_codigos):\n",
    "#     # Inicializar las nuevas columnas\n",
    "#     df_codigos['existente'] = 'no'\n",
    "#     df_codigos['archivo'] = ''\n",
    "#     df_codigos['fila'] = -1\n",
    "#     df_codigos['extension'] = ''\n",
    "    \n",
    "#     # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "#     if ruta.endswith('.xlsx'):\n",
    "#         # Si es un archivo Excel, procesarlo directamente\n",
    "#         df = pd.read_excel(ruta)\n",
    "#         procesar_dataframe(df, df_codigos, os.path.basename(ruta))\n",
    "#     else:\n",
    "#         # Si es una carpeta, procesar todos los archivos CSV\n",
    "#         archivos_csv = glob(os.path.join(ruta, '*.csv'))\n",
    "#         for archivo in archivos_csv:\n",
    "#             # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "#             for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "#                 procesar_dataframe(chunk, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "#     return df_codigos\n",
    "\n",
    "\n",
    "# def procesar_dataframe(df, df_codigos, nombre_archivo):\n",
    "#     for idx, row_codigos in df_codigos.iterrows():\n",
    "#         codigo = row_codigos['Codigo']\n",
    "#         matches = df[df['contenido'].apply(lambda x: buscar_codigo(codigo, x))]\n",
    "        \n",
    "#         if not matches.empty:\n",
    "#             primera_coincidencia = matches.iloc[0]\n",
    "#             df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension']] = [\n",
    "#                 'si', \n",
    "#                 nombre_archivo, \n",
    "#                 primera_coincidencia.name, \n",
    "#                 primera_coincidencia['extension']\n",
    "#             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_codigo(codigo, cadena):\n",
    "    # Asegurarse de que ambos sean cadenas antes de aplicar strip() y upper()\n",
    "    if pd.isna(codigo) or not isinstance(codigo, str):\n",
    "        return False  # Omitir si el código no es una cadena\n",
    "    if pd.isna(cadena) or not isinstance(cadena, str):\n",
    "        return False  # Omitir si la cadena no es válida\n",
    "    \n",
    "    # Convertir a mayúsculas y eliminar espacios en blanco\n",
    "    return codigo.strip().upper() in cadena.strip().upper()\n",
    "\n",
    "def procesar_archivos(ruta, df_codigos):\n",
    "    # Inicializar las nuevas columnas\n",
    "    df_codigos['existente'] = 'no'\n",
    "    df_codigos['archivo'] = ''\n",
    "    df_codigos['fila'] = -1\n",
    "    df_codigos['extension'] = ''\n",
    "    df_codigos['excepcion'] = ''  # Nueva columna para excepciones\n",
    "    \n",
    "    # Verificar si la ruta es un archivo Excel o una carpeta\n",
    "    if ruta.endswith('.xlsx'):\n",
    "        # Si es un archivo Excel, procesarlo directamente\n",
    "        df = pd.read_excel(ruta)\n",
    "        procesar_dataframe(df, df_codigos, os.path.basename(ruta))\n",
    "    else:\n",
    "        # Si es una carpeta, procesar todos los archivos CSV\n",
    "        archivos_csv = glob(os.path.join(ruta, '*.csv'))\n",
    "        for archivo in archivos_csv:\n",
    "            # Leer el archivo CSV en chunks para optimizar la memoria\n",
    "            for chunk in pd.read_csv(archivo, chunksize=10000, header=None, names=['contenido', 'extension']):\n",
    "                procesar_dataframe(chunk, df_codigos, os.path.basename(archivo))\n",
    "    \n",
    "    return df_codigos\n",
    "\n",
    "def procesar_dataframe(df, df_codigos, nombre_archivo):\n",
    "    for idx, row_codigos in df_codigos.iterrows():\n",
    "        codigo_primario = row_codigos['Codigo']\n",
    "        codigo_secundario = row_codigos['Resto']  # Asumimos que esta es la segunda columna\n",
    "        \n",
    "        # Buscar coincidencia en la primera columna (Código primario)\n",
    "        matches_primario = df[df['contenido'].apply(lambda x: buscar_codigo(codigo_primario, x))]\n",
    "        \n",
    "        if not matches_primario.empty:\n",
    "            primera_coincidencia = matches_primario.iloc[0]\n",
    "            df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension']] = [\n",
    "                'si', \n",
    "                nombre_archivo, \n",
    "                primera_coincidencia.name, \n",
    "                primera_coincidencia['extension']\n",
    "            ]\n",
    "        else:\n",
    "            # Si no se encuentra el código primario, buscar en la segunda columna (Código secundario)\n",
    "            matches_secundario = df[df['contenido'].apply(lambda x: buscar_codigo(codigo_secundario, x))]\n",
    "            \n",
    "            if not matches_secundario.empty:\n",
    "                primera_coincidencia = matches_secundario.iloc[0]\n",
    "                df_codigos.loc[idx, ['existente', 'archivo', 'fila', 'extension', 'excepcion']] = [\n",
    "                    'si', \n",
    "                    nombre_archivo, \n",
    "                    primera_coincidencia.name, \n",
    "                    primera_coincidencia['extension'],\n",
    "                    primera_coincidencia['contenido']  # Guardar el contenido de la celda como excepción\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Codigo_Secundario'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desarrollo/comparacion-codigos-nas/my_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Codigo_Secundario'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ruta_archivo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/original/nas_csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df_codigos_actualizado \u001b[38;5;241m=\u001b[39m \u001b[43mprocesar_archivos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_archivo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_codigos\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m, in \u001b[0;36mprocesar_archivos\u001b[0;34m(ruta, df_codigos)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m archivo \u001b[38;5;129;01min\u001b[39;00m archivos_csv:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# Leer el archivo CSV en chunks para optimizar la memoria\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(archivo, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenido\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextension\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 30\u001b[0m             \u001b[43mprocesar_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_codigos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchivo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_codigos\n",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m, in \u001b[0;36mprocesar_dataframe\u001b[0;34m(df, df_codigos, nombre_archivo)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row_codigos \u001b[38;5;129;01min\u001b[39;00m df_codigos\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     36\u001b[0m     codigo_primario \u001b[38;5;241m=\u001b[39m row_codigos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodigo\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m     codigo_secundario \u001b[38;5;241m=\u001b[39m \u001b[43mrow_codigos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCodigo_Secundario\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Asumimos que esta es la segunda columna\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Buscar coincidencia en la primera columna (Código primario)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     matches_primario \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontenido\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: buscar_codigo(codigo_primario, x))]\n",
      "File \u001b[0;32m~/Desarrollo/comparacion-codigos-nas/my_env/lib/python3.9/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/Desarrollo/comparacion-codigos-nas/my_env/lib/python3.9/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/Desarrollo/comparacion-codigos-nas/my_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Codigo_Secundario'"
     ]
    }
   ],
   "source": [
    "ruta_archivo = 'data/original/nas_csv'\n",
    "\n",
    "df_codigos_actualizado = procesar_archivos(ruta_archivo, df_codigos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
